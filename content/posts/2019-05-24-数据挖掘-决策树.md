---
title: 数挖 | 决策树
date: 2019-05-24 21:42:43
categories: ["数据流水线"]
tags: 
    - 机器学习
    - 决策树
---

## 基本流程

决策树（decision tree）是一类常见的机器学习方法。顾名思义，决策树是基于树结构（可能是二叉树或多叉树）来进行决策的，这恰是人类在面临决策问题时一种很自然的处理机制。

使用决策树进行决策的过程就是从**根节点**开始，测试待分类项中相应的特征属性，并按照其值选择输出**分支**，直到到达**叶子节点**，将叶子节点存放的类别作为决策结果。模型基本结构如下：

![决策树模型基本架构](https://blog-1255524710.cos.ap-beijing.myqcloud.com/images/data-mining-decision-tree/%E5%86%B3%E7%AD%96%E6%A0%91.jpg)

> 一棵决策树包含一个根结点、若干个内部结点和若干个叶结点：
>
> - 叶结点对应于决策结果，其他每个结点则对应于一个属性测试；
>
> - 每个结点包含的样本集合根据属性测试的结果被划分到子结点中；
>
> - 根结点包含样本全集。
>
> 从根结点到每个叶结点的路径对应了一个判定测试序列。

决策树学习的目的是为了产生一棵泛化能力强，即处理未见示例能力强的决策树，其基本流程遵循简单且直观的「分而治之」（divide-and-conquer）策略，如下图所示。

![决策树算法基本流程](https://blog-1255524710.cos.ap-beijing.myqcloud.com/images/data-mining-decision-tree/Snipaste_2019-05-24_22-42-08.png)

从上述流程可看出，决策树的生成是一个递归过程，所谓决策树的构造就是不断进行属性选择度量确定各个特征属性之间的拓扑结构。在决策树基本算法中，有三种情形会导致递归返回：

1. 当前结点包含的样本全属于同一类别，无需划分；
2. 当前属性集为空，或是所有样本在所有属性上取值相同，无法划分；
3. 当前结点包含的样本集合为空，不能划分。

在第（2）种情形下，我们把当前结点标记为叶结点，并将其类别设定为该结点所含样本最多的类别；在第（3）种情形下，同样把当前结点标记为叶结点，但将其类别设定为其父结点所含样本最多的类别。

> 注意后两种情形的处理实质不同：情形（2）是在利用当前结点的后验分布，而情形（3）则是把父结点的样本分布作为当前结点的先验分布。

由算法流程可以看出，<u>***决策树学习的关键是第 8 行，即如何选择最优划分属性***</u>，所谓划分属性就是在某个节点处按照某一特征属性的不同划分构造不同的分支，其目标是让各个分裂子集尽可能地「纯」。尽可能「纯」就是尽量让一个分裂子集中待分类项属于同一类别。分裂属性分为三种不同的情况：

1. 属性是离散值且不要求生成二叉决策树，此时用属性的每一个划分作为一个分支。
2. 属性是离散值且要求生成二叉决策树，此时使用属性划分的一个子集进行测试，按照“属于此子集”和“不属于此子集”分成两个分支。
3. 属性是连续值，此时确定一个值作为分裂点 split_point，按照 >split_point 和 <=split_point 生成两个分支。

构造决策树的关键性内容是进行属性选择度量，属性选择度量是一种特征选择准则，是将给定的类标记的训练集合的数据集“最好”地分成个体类的启发式方法，它决定了拓扑结构及分裂点 split_point 的选择。下一节将就如何进行特征选择进行讨论。

## 特征选择

一般而言，随着划分过程不断进行，我们希望决策树的分支结点所包含的样本尽可能属于同一类别，即结点的**纯度**（purity）越来越高。一个数据集的纯度的度量有多种方式，这里主要介绍信息增益、信息增益率和基尼系数这三种常用指标。

### 信息增益

#### 熵与条件熵

「信息熵」（information entropy）是度量样本集合纯度最常用的一种指标。在信息论与概率统计中，熵（entropy）是表示随机变量不确定性的度量。设 $X$ 是一个取有限个值的离散随机变量，其概率分布为
$$
P\left(X=x_{i}\right)=p_{i}, \quad i=1,2, \cdots, n
$$
则随机变量 $X$ 的熵定义为
$$
H(X)=-\sum_{i=1}^{n} p_{t} \log p_{i}
$$
上式中，若 $p_i=0$，则定义 $0log0=0$。通常，上式中的对数以 2 为底或以 $e$ 为底（自然对数），这时熵的单位分别称作比特（bit）或纳特（nat）。由定义可知，熵只依赖于 $X$ 的分布，而与 $X$ 的取值无关，所以也可将 $X$ 的熵记作 $H(p)$，即
$$
H(p)=-\sum_{i=1}^{n} p_{i} \log p_{i}
$$
<u>***熵越大，随机变量的不确定性就越大，代表纯度越低***</u>。从定义可验证
$$
0 \leqslant H(p) \leqslant \log n
$$
当随机变量只取两个值，例如 $1,0$ 时，即 $X$ 的分布为
$$
P(X=1)=p, \quad P(X=0)=1-p, \quad 0 \leqslant p \leqslant 1
$$
熵为
$$
H(p)=-p \log _{2} p-(1-p) \log _{2}(1-p)
$$
这时，熵 $H(p)$ 随概率 $p$ 变化的曲线如下图所示（单位为比特）：

![分布为贝努利分布时熵与概率的关系](https://blog-1255524710.cos.ap-beijing.myqcloud.com/images/data-mining-decision-tree/Snipaste_2019-05-25_10-47-15.png)

当 $p=0$ 或 $p=1$ 时 $H(p)=0$，随机变量完全没有不确定性。当 $p=0.5$ 时 $H(p)=1$，熵取值最大，随机变量不确定性最大。

设有随机变量 $(X,Y)$，其联合概率分布为
$$
P\left(X=x_{i}, Y=y_{j}\right)=p_{ij}, \quad i=1,2, \cdots, n, \quad j=1,2, \cdots, m
$$
<u>***条件熵 $H(Y | X)$ 表示在已知随机变量 $X$ 的条件下随机变量 $Y$ 的不确定性***</u>。随机变量 $X$ 给定的条件下随机变量 $Y$ 的条件熵（conditional entropy）$H(YlX)$，定义为 $X$ 给定条件下 $Y$ 的条件概率分布的熵对 $X$ 的数学期望
$$
H(Y | X)=\sum_{i=1}^{n} p_{i} H\left(Y | X=x_{i}\right)
$$
这里，$
p_{i}=P\left(X=x_{i}\right), \quad i=1,2, \cdots, n
$。

当熵和条件熵中的概率由数据估计（特别是极大似然估计）得到时，所对应的熵与条件熵分别称为经验熵（empirical entropy）和经验条件熵（empirical conditional entropy）。此时，如果有 $0$ 概率，令 $0log0=0$.

#### 信息增益

信息增益（information gain）表示得知特征 $X$ 的信息而使得类 $Y$ 的信息的不确定性减少的程度。

> **定义：**特征 $A$ 对训练数据集 $D$ 的信息增益 $g(D,A)$，定义为集合 $D$ 的经验熵 $H(D)$ 与特征 $A$ 给定条件下 $D$ 的经验条件熵$H(D|A)$ 之差，即
> $$
> g(D, A)=H(D)-H(D | A)
> $$

一般地，熵 $H(Y)$ 与条件熵 $H(Y|X)$ 之差称为互信息（mutual information）。决策树学习中的信息增益等价于训练数据集中类与特征的互信息。决策树学习应用信息增益准则选择特征。给定训练数据集 $D$ 和特征 $A$，经验熵 $H(D)$ 表示对数据集 $D$ 进行分类的不确定性。而经验条件熵 $H(D|A)$ 表示在特征 $A$ 给定的条件下对数据集 $D$ 进行分类的不确定性。那么它们的差，即信息增益，就表示由于特征 $A$ 而使得对数据集 $D$ 的分类的不确定性减少的程度。显然，对于数据集 $D$ 而言，信息增益依赖于特征，不同的特征往往具有不同的信息增益。<u>***信息增益大的特征具有更强的分类能力***</u>。

上面一堆概念，大家估计比较晕，用下面这个图很容易明白它们的关系。左边的椭圆代表 $H(X)$，右边的椭圆代表 $H(Y)$，中间重合的部分就是我们的互信息或者信息增益 $I(X,Y)$，左边的椭圆去掉重合部分就是 $H(X|Y)$，右边的椭圆去掉重合部分就是 $H(Y|X)$。两个椭圆的并就是 $H(X,Y)$。

![](https://blog-1255524710.cos.ap-beijing.myqcloud.com/images/data-mining-decision-tree/Snipaste_2019-05-25_11-24-15.png)

> 根据信息增益准则的特征选择方法：对训练数据集（或子集）$D$，计算其每个特征的信息增益，并比较它们的大小，选择信息增益最大的特征。

设训练数据集为 $D$，$|D|$ 表示其样本容量，即样本个数。设有 $K$ 个类 $C_k, k = 1,2, \cdots, K$，$\left|C_{k}\right|$ 为属于类 $C_k$ 的样本个数，$\sum_{k=1}^{K}\left|C_{k}\right|=|D|$。设特征 $A$ 有 $n$ 个不同的取值 $\\{a_1,a_2,\cdots,a_n\\}$，根据特征 $A$ 的取值将 $D$ 划分为 $n$ 个子集 $D_{1}, D_{2}, \cdots, D_{n}$，，$|D_i|$ 为 $D_i$ 的样本个数，$\sum_{i=1}^{n}\left|D_{i}\right|=|D|$。记子集 $D_i$ 中属于类 $C_k$ 的样本的集合为 $D_{ik}$，即 $D_{ik}=D_{i} \cap C_{k}$，$|D_{ik}|$ 为 $D_{ik}$ 的样本个数。于是信息增益的算法如下：



![信息增益算法](https://blog-1255524710.cos.ap-beijing.myqcloud.com/images/data-mining-decision-tree/Snipaste_2019-05-25_11-17-00.png)

一般而言，信息增益越大，则意味着使用属性 $a$ 来进行划分所获得的“纯度提升”越大。因此，我们可用信息增益来进行决策树的划分属性选择，即在决策树算法第 8 行选择属性 $a_{*}=\underset{a \in A}{\arg \max } \operatorname{Gain}(D, a)$。著名的 ID3 决策树学习算法[Quinlan，1986] 就是以信息增益为准则来选择划分属性。

### 信息增益率

实际上，信息增益准则对可取值数目较多的属性有所偏好，为减少这种偏好可能带来的不利影响，著名的 C4.5 决策树算法[Quinlan，1993] 不直接使用信息增益，而是使用“增益率”（gain ratio）来选择最优划分属性。
$$
(D, a)=\frac{\operatorname{Gain}(D, a)}{\operatorname{IV}(a)}
$$
其中
$$
\mathrm{IV}(a)=-\sum_{v=1}^{V} \frac{\left|D^{v}\right|}{|D|} \log _{2} \frac{\left|D^{v}\right|}{|D|}
$$
称为属性 $a$ 的「固有值」（intrinsic value）[Quinlan，1993]。属性 $a$ 的可能取值数目越多（即 $V$ 越大），则 $IV(a)$ 的值通常会越大。特征数越多的特征对应的特征熵越大，它作为分母，可以校正信息增益容易偏向于取值较多的特征的问题。

需注意的是，增益率准则对可取值数目较少的属性有所偏好，因此，C4.5 算法并不是直接选择增益率最大的候选划分属性，而是使用了一个启发式[Quinlan，1993]：先从候选划分属性中找出信息增益高于平均水平的属性，再从中选择增益率最高的。

### 基尼系数

CART 决策树[Breiman et al.，1984] 使用「基尼指数」（Gini index）来选择划分属性。数据集 $D$ 的纯度可用基尼值来度量：

$$
\begin{aligned} \operatorname{Gini}(D) &=\sum_{k=1}^{ | \mathcal{Y |}} \sum_{k^{\prime} \neq k} p_{k} p_{k^{\prime}} \\ &=1-\sum_{k=1}^{|\mathcal{Y}|} p_{k}^{2} \end{aligned}
$$
直观来说，$Gini(D)$ 反映了从数据集 $D$ 中随机抽取两个样本，其类别标记不一致的概率。因此，$Gini(D)$ 越小，则数据集 $D$ 的纯度越高。

属性 $a$ 的基尼指数定义为
$$
\operatorname{Gini} \operatorname{index}(D, a)=\sum_{v=1}^{V} \frac{\left|D^{v}\right|}{|D|} \operatorname{Gini}\left(D^{v}\right)
$$
于是，我们在候选属性集合 $A$ 中，选择那个使得划分后基尼指数最小的属性作为最优划分属性，即 $a_{*}=\underset{a \in A}{\arg \min } \operatorname{Gini}$ index $(D, a)$.

可以比较下基尼系数表达式和熵模型的表达式，二次运算是不是比对数简单很多？尤其是二类分类的计算，更加简单。但是简单归简单，和熵模型的度量方式比，基尼系数对应的误差有多大呢？对于二类分类，基尼系数和熵之半的曲线如下：

![评估指标对比图](https://blog-1255524710.cos.ap-beijing.myqcloud.com/images/data-mining-decision-tree/1042406-20161111105202170-1563882835.jpg)

从上图可以看出，基尼系数和熵之半的曲线非常接近，仅仅在 45 度角附近误差稍大。因此，基尼系数可以做为熵模型的一个近似替代。而 CART 分类树算法就是使用的基尼系数来选择决策树的特征。同时，为了进一步简化，CART 分类树算法每次仅仅对某个特征的值进行二分，而不是多分，这样 CART 分类树算法建立起来的是二叉树，而不是多叉树。这样一来可以进一步简化基尼系数的计算，而且可以建立一个更加优雅的二叉树模型。

## 生成算法

![决策树算法](https://blog-1255524710.cos.ap-beijing.myqcloud.com/images/data-mining-decision-tree/%E5%86%B3%E7%AD%96%E6%A0%91%E7%AE%97%E6%B3%95.png)

### ID3

ID3 名字中的 ID 是 lterative Dichotomiser（迭代二分器）的简称。ID3 算法就是用信息增益大小来判断当前节点应该用什么特征来构建决策树，用计算出的信息增益最大的特征来建立决策树的当前节点。这里我们举一个信息增益计算的具体的例子，比如有 15 个样本 D，输出为 0 或者 1。其中有 9 个输出为 1， 6 个输出为 0。 样本中有个特征 A，取值为 A1，A2 和 A3。在取值为 A1 的样本的输出中，有 3 个输出为 1， 2 个输出为 0，取值为 A2 的样本输出中，2 个输出为 1，3 个输出为 0， 在取值为 A3 的样本中，4 个输出为 1，1 个输出为 0。

样本 D 的熵为：$H(D)=-\left(\frac{9}{15} \log _{2} \frac{9}{15}+\frac{6}{15} \log _{2} \frac{6}{15}\right)=0.971$

样本 D 在特征下的条件熵为：$H(D | A)=\frac{5}{15} H(D 1)+\frac{5}{15} H(D 2)+\frac{5}{15} H(D 3)=0.888$

对应的信息增益为：$I(D, A)=H(D)-H(D | A)=0.083$

具体算法过程大概是这样的。

输入的是 m 个样本，样本输出集合为 D，每个样本有 n 个离散特征，特征集合即为 A，输出为决策树 T。

算法的过程为：

1. 初始化信息增益的阈值 $\epsilon$ ；
2. 判断样本是否为同一类输出 $D_{i}$，如果是则返回单节点树 $T$。标记类别为 $D_{i}$ ；
3. 判断特征是否为空，如果是则返回单节点树 $T$，标记类别为样本中输出类别 $D$ 实例数最多的类别；
4. 计算 $A$ 中的各个特征（一共 $n$ 个）对输出 $D$ 的信息增益，选择信息增益最大的特征 $A_g$ ；
5. 如果 $A_g$ 的信息增益小于阈值 $\epsilon$ ，则返回单节点树 $T$，标记类别为样本中输出类别 $D$ 实例数最多的类别；
6. 否则，按特征 $A_g$ 的不同取值 $A_{gi}$ 将对应的样本输出 $D$ 分成不同的类别 $D_i$。每个类别产生一个子节点，对应特征值为 $A_{gi}$。返回增加了节点的数 $T$；
7. 对于所有的子节点，令 $D=D_{i}, A=A-A_{g}$ 递归调用 2-6 步，得到子树 $T_{i}$ 并返回。

ID3 算法一出现就因其简洁和高效引起了轰动，但是，ID3 算法虽然提出了新思路，还是有很多值得改进的地方。

> **决策树 ID3 算法的不足：**
>
> 1. ID3 没有考虑连续特征，比如长度，密度都是连续值，无法在 ID3 运用，这大大限制了 ID3 的应用；
> 2. ID3 采用信息增益大的特征优先建立决策树的节点。很快就被人发现，在相同条件下，取值比较多的特征比取值少的特征信息增益大。比如一个变量有 2 个值，各为 1/2，另一个变量为 3 个值，各为 1/3，其实它们都是完全不确定的变量，但是取 3 个值的比取 2 个值的信息增益大。
> 3.  ID3 算法对于缺失值的情况没有做考虑；
> 4. 没有考虑过拟合的问题。

ID3 算法的作者昆兰基于上述不足，对 ID3 算法做了改进，这就是 C4.5 算法，也许你会问，为什么不叫 ID4，ID5 之类的名字呢?那是因为决策树太火爆，他的 ID3 一出来，别人二次创新，很快就占了 ID4，ID5，所以他另辟蹊径，取名 C4.0 算法，后来的进化版为 C4.5 算法。

### C4.5

- 对于 ID3 第一个问题，不能处理连续特征， C4.5 的思路是<u>将连续的特征离散化</u>。

  比如 $m$ 个样本的连续特征 $A$ 有 $m$ 个，从小到大排列为 $a_{1}, a_{2}, \dots, a_{m}$，则 C4.5 取相邻两样本值的平均数，一共取得 $m-1$ 个划分点，其中第 $i$ 个划分点 $T_{i}$ 表示为：$T_{i}=\frac{a_{i}+a_{i+1}}{2}$。对于这 $m-1$ 个点，分别计算以该点作为二元分类点时的信息增益。选择信息增益最大的点作为该连续特征的二元离散分类点。比如取到的增益最大的点为 $a_{t}$，则小于 $a_{t}$ 的值为类别 1，大于 $a_{t}$ 的值为类别 2，这样我们就做到了连续特征的离散化。要注意的是，与离散属性不同的是，如果当前节点为连续属性，则该属性后面还可以参与子节点的产生选择过程。

- 对于第二个问题，ID3 算法以信息增益作为标准容易偏向于取值较多的特征，C4.5 <u>引入信息增益率作为特征选择标准</u>来解决这个问题。

- 对于第三个缺失值处理的问题，主要需要解决的是两类问题，一是在样本某些特征缺失的情况下选择划分的属性，二是选定了划分属性，对于在该属性上缺失特征的样本的处理。

  第一个子问题，对于某一个有缺失特征值的特征 A，C4.5 的思路是将数据分成两部分，对每个样本设置一个权重（初始可以都为 1），然后划分数据，一部分是有特征值 A 的数据 D1，另一部分是没有特征 A 的数据 D2。然后对于没有缺失特征 A 的数据集 D1 来和对应的 A 特征的各个特征值一起计算加权重后的信息增益率，最后乘上一个系数，这个系数是无特征 A 缺失的样本加权后所占加权总样本的比例；对于第二个子问题，可以将缺失特征的样本同时划分入所有的子节点，不过将该样本的权重按各个子节点样本的数量比例来分配。比如缺失特征 A 的样本 a 之前权重为 1，特征 A 有 3 个特征值A1,  A2,  A3。 3 个特征值对应的无缺失 A 特征的样本个数为 2, 3, 4。则 a 同时划分入 A1, A2, A3。对应权重调节为 2/9, 3/9, 4/9。

- 对于第四个问题，C4.5 引入了<u>正则化系数</u>进行初步的剪枝，具体方法在剪枝部分进行介绍。

C4.5 虽然改进或者改善了 ID3 算法的几个主要的问题，仍然有优化的空间。

> 1. 由于决策树算法非常容易过拟合，因此对于生成的决策树必须要进行剪枝。剪枝的算法有非常多，C4.5 的剪枝方法有优化的空间。思路主要是两种，一种是预剪枝，即在生成决策树的时候就决定是否剪枝。另一个是后剪枝，即先生成决策树，再通过交叉验证来剪枝。
> 2. C4.5 生成的是多叉树，即一个父节点可以有多个节点。很多时候，在计算机中二叉树模型会比多叉树运算效率高。如果采用二叉树，可以提高效率。
> 3. C4.5 只能用于分类，如果能将决策树用于回归的话可以扩大它的使用范围。
> 4. C4.5 由于使用了熵模型，里面有大量的耗时的对数运算，如果是连续值还有大量的排序运算，处理成本耗时较高。

### CART

CART 是 Classification and Regression Tree 的简称，这是一种著名的决策树学习算法，分类和回归任务都可用。

- 对于 CART 分类树连续值的处理问题，其思想和 C4.5 是相同的，都是将连续的特征离散化。唯一的区别在于在选择划分点时的度量方式不同，C4.5 使用的是信息增益率，则 CART 分类树使用的是基尼系数。

- CART 分类树对于离散值的处理问题，采用的思路是不停的二分离散特征。

  回忆下 ID3 或者 C4.5，如果某个特征 A 被选取建立决策树节点，如果它有 A1, A2, A3 三种类别，我们会在决策树上一下建立一个三叉的节点，这样导致决策树是多叉树。但是 CART 分类树使用的方法不同，它采用的是不停的二分，还是这个例子，CART 分类树会考虑把 A 分成 {A1} 和 {A2, A3}，{A2} 和 {A1, A3}，{A3} 和 {A1, A2} 三种情况，找到基尼系数最小的组合，比如 {A2} 和 {A1, A3}，然后建立二叉树节点，一个节点是 A2 对应的样本，另一个节点是 {A1, A3} 对应的节点。同时，由于这次没有把特征 A 的取值完全分开，后面我们还有机会在子节点继续选择到特征 A 来划分A1 和 A3。这和 ID3 或者 C4.5 不同，在 ID3 或者 C4.5 的一棵子树中，离散特征只会参与一次节点的建立。

CART 是在给定输入随机变量 $X$ 条件下输出随机变量 $Y$ 的条件概率分布的学习方法。CART 假设决策树是二叉树，内部结点特征的取值为「是」和「否」，左分支是取值为「是」的分支，右分支是取值为「否」的分支。这样的决策树等价于递归地二分每个特征，将输入空间即特征空间划分为有限个单元，并在这些单元上确定预测的概率分布，也就是在输入给定的条件下输出的条件概率分布。

> CART 算法由以下两步组成：
>
> 1. 决策树生成：基于训练数据集生成决策树，生成的决策树要尽量大；
> 2. 决策树剪枝：用验证数据集对已生成的树进行剪枝并选择最优子树，这时用损失函数最小作为剪枝的标准。

本节主要介绍决策树的生成算法，剪枝算法在下一节单独介绍。决策树的生成就是递归地构建二叉决策树的过程，对回归树用平方误差最小化准则，对分类树用基尼指数（Gini index）最小化准则，进行特征选择，生成二叉树。

#### 回归树的生成

假设 $X$ 与 $Y$ 分别为输入和输出变量，并且 $Y$ 是连续变量，给定训练数据集
$$
D=\left\{\left(x_{1}, y_{1}\right),\left(x_{2}, y_{2}\right), \cdots,\left(x_{N}, y_{N}\right)\right\}
$$
考虑如何生成回归树。

一个回归树对应着输入空间（即特征空间）的一个划分以及在划分的单元上的输出值。假设已将输入空间划分为 $M$ 个单元 $ R_{1}, R_{2}, \cdots, R_{M} $ ，并且在每个单元 $R_m$ 上有一个固定的输出值 $c_m$，于是回归树模型可表示为
$$
f(x)=\sum_{m=1}^{M} c_{m} I\left(x \in R_{m}\right)
$$
当输入空间的划分确定时，可以用平方误差 $\sum_{x_{i} \in R_{m}}\left(y_{i}-f\left(x_{i}\right)\right)^{2}$ 来表示回归树对于训练数据的预测误差，用平方误差最小的准则求解每个单元上的最优输出值。易知，单元 $R_m$ 上的 $c_m$ 的最优值 $\hat{c}_{m}$ 是 $R_m$ 上的所有输入实例 $x_i$ 对应的输出 $y_i$ 的均值，即
$$
\hat{c}_{m}=\operatorname{ave}\left(y_{i} | x_{i} \in R_{m}\right)
$$
问题是怎样对输入空间进行划分。这里采用启发式的方法，选择第 $j$ 个变量 $x^{(j)}$ 和它取的值 $s$，作为切分变量（spliting variable）和切分点（splitting point），并定义两个区域：
$$
R_{1}(j, s)=\left\{x | x^{(j)} \leqslant s\right\} \quad , \quad R_{2}(j, s)=\left\{x | x^{(j)}>s\right\}
$$
然后寻找最优切分变量 $j$ 和最优切分点 $s$。具体地，求解
$$
\min _{j,s} [\min _{c_{1}} \sum_{x_{i} \in R_{1}(j, s)}\left(y_{i}-c_{1}\right)^{2} + \min _{c_{2}} \sum_{x_{i} \in R_{2}(j, s)}\left(y_{i}-c_{2}\right)^{2}]
$$
对固定输入变量 $j$ 可以找到最优切分点 $s$。
$$
\hat{c}_{1}=\operatorname{ave}\left(y_{i} | x_{i} \in R_{1}(j, s)\right) \quad , \quad \hat{c}_{2}=\operatorname{ave}\left(y_{i} | x_{i} \in R_{2}(j, s)\right)
$$
遍历所有输入变量，找到最优的切分变量 $j$，构成一个对 $(j，s)$。依此将输入空间划分为两个区域。接着，对每个区域重复上述划分过程，直到满足停止条件为止。这样就生成一棵回归树。这样的回归树通常称为最小二乘回归树（least squares regression tree），现将算法整理如图：

![回归树生成算法](https://blog-1255524710.cos.ap-beijing.myqcloud.com/images/data-mining-decision-tree/Snipaste_2019-05-27_15-11-52.png)



#### 分类树的生成

分类树用「基尼指数」选择最优特征，同时决定该特征的最优二值切分点。

输入：训练数据集 $D$，停止计算的条件

输出：CART 决策树

根据训练数据集，从根结点开始，递归地对每个结点进行以下操作，构建二叉决策树：

1. 设结点的训练数据集为 $D$，计算现有特征对该数据集的基尼指数。此时，对每一个特征 $A$，对其可能取的每个值 $a$，根据样本点对 $A=a$ 的测试为「是」或
   「否」将 $D$ 分割成 $D_{1}$ 和 $D_{2}$ 两部分，利用公式计算 $A=a$ 时的基尼指数；
2. 在所有可能的特征 $A$ 以及它们所有可能的切分点 $a$ 中，选择基尼指数最小的特征及其对应的切分点作为最优特征与最优切分点。依最优特征与最优切分点，从现结点生成两个子结点，将训练数据集依特征分配到两个子结点中去；
3. 对两个子结点递归地调用步骤 1，2，直至满足停止条件；
4. 生成 CART 决策树。

算法停止计算的条件是结点中的样本个数小于预定阀值，或样本集的基尼指数小于预定阈值（样本基本属于同一类），或者没有更多特征。

下图展现一个小例子：

![回归树基尼系数计算示例](https://blog-1255524710.cos.ap-beijing.myqcloud.com/images/data-mining-decision-tree/Snipaste_2019-05-27_15-21-43.png)

## 剪枝处理

剪枝（pruning）是决策树学习算法对付<u>**过拟合**</u>的主要手段。在决策树学习中，为了尽可能正确分类训练样本，结点划分过程将不断重复，有时会造成决策树分支过多，这时就可能因训练样本学得「太好」了，以致于把训练集自身的一些特点当作所有数据都具有的一般性质而导致过拟合。因此，可通过主动去掉一些分支来降低过拟合的风险。

决策树剪枝的基本策略有「预剪枝」（prepruning）和「后剪枝」（post-
pruning）[Quinlan，1993]。预剪枝是指在决策树生成过程中，对每个结点在划分前先进行估计，若当前结点的划分不能带来决策树泛化性能提升，则停止划分并将当前结点标记为叶结点；后剪枝则是先从训练集生成一棵完整的决策树，然后自底向上地对非叶结点进行考察，若将该结点对应的子树替换为叶结点能带来决策树泛化性能提升，则将该子树替换为叶结点。那么，如何判断决策树泛化性能是否提升呢？这可使用特征选择中介绍的性能评估方法，可采用留出法，即预留一部分数据用作「验证集」以进行性能评估。

### 预剪枝





### 后剪枝







## 算法小结

| 算法 | 支持模型  | 树结构 | 特征选择          | 连续值处理 | 缺失值处理 | 剪枝   |
| ---- | --------- | ------ | ----------------- | ---------- | ---------- | ------ |
| ID3  | 分类      | 多叉树 | 信息增益          | 不支持     | 不支持     | 不支持 |
| C4.5 | 分类      | 多叉树 | 信息增益率        | 支持       | 支持       | 支持   |
| CART | 分类/回归 | 二叉树 | 基尼系数/均方误差 | 支持       | 支持       | 支持   |

### 决策树算法优点

1. 简单直观，生成的决策树很直观；
2. 基本不需要预处理，不需要提前归一化，处理缺失值；
3. 使用决策树预测的代价是 $O(log_{2}m)$，$m$ 为样本数；
4. 既可以处理离散值也可以处理连续值，很多算法只是专注于离散值或者连续值；
5. 可以处理多维度输出的分类问题；
6. 相比于神经网络之类的黑盒分类模型，决策树在逻辑上可以得到很好的解释；
7. 可以交叉验证的剪枝来选择模型，从而提高泛化能力；
8. 对于异常点的容错能力好，健壮性高。

### 决策树算法缺点

1. 决策树算法容易过拟合，导致泛化能力不强，可通过设置节点最少样本数量和限制决策树深度来改进；
2. 决策树会因为样本发生一点点的改动，就会导致树结构的剧烈改变，可以通过集成学习之类的方法解决；
3. 寻找最优的决策树是一个 NP 难的问题，一般通过启发式方法，易陷入局部最优，可通过集成学习之类的方法来改善；
4. 有些比较复杂的关系，决策树很难学习，比如异或，一般这种关系可以换神经网络分类方法来解决；
5. 如果某些特征的样本比例过大，生成决策树容易偏向于这些特征，可通过调节样本权重来改善。

### 深入探讨

#### 连续值处理



#### 缺失值处理



#### 多变量决策树



## 代码清单



### Python



### R









> ### 参考文献：
>
> 1. [周志华，机器学习，清华大学出版社，2016-1-1．73 - 95．](<https://book.douban.com/subject/26708119/>)
> 2. [李舰，统计学习方法，清华大学出版社，2012-3．55－76．](<https://book.douban.com/subject/10590856/>)
> 3. [决策树算法原理——刘建平Pinard的博客．](<https://www.cnblogs.com/pinard/p/6050306.html>)