---
title: "回归分析系列2-简单线性回归"
date: 2018-03-11T10:00:00+08:00
categories: [知识库]
tags: 
    - 技术
    - 回归分析
    - 统计
comment: true
toc: true
---

<center><i>
    与大多数统计方法一样，回归也是一种简化数据的技术。<br>回归分析利用变量间的简单函数关系，用自变量对困变量进行"预测"，使"预测值"尽可能地接近因变量的"观测值" 。
</i></center>

<!--more-->

## 本节综述
简单线性回归属于回归分析中最基础的一部分内容，仅仅涉及到两个变量之间的线性关系，但其作为学习多元回归乃至其他更复杂统计方法的基础，所以本节将先从理解"回归"这一概念人手，并讨论只有一个因变量和一个自变量的简单线性回归模型。

## 理解回归
如何根据回归模型的构成形式理解回归模型的现实意义呢？在此，我们提出理解回归的三种视角：
<div class="note info"><p>
    1. 因果性：观测项 = 机制项 + 干扰项<br>
    2. 预测性：观测项 = 预测项 + 误差项<br>
    3. 描述性：观测项 = 概括项 + 残差项
</p></div>

这三种理解方式提供了定量分析的三种不同视角：

- 第一种方式接近于 **古典计量经济学** 的视角，这种方法试图找出具有决定性的模型并以此发现数据产生的机制。但当前更多的方法论研究者认为，所谓的"真实"模型并不存在，好的模型只是相对于其他模型而言更实用、更有意义或者更接近真实。
- 第二种方式适用于 **工程学** 领域，它通常用于在已知一组自变量和因变量之间的关系后，应用新的数据给出有用的预测回答。这一理解方法的特点是我们只是通过经验规律来做预测，而对因果关系的机制不感兴趣或不在乎。
- 第三种方式反映了当今 **定量社会科学和统计学** 的主流观点。它希望在不曲解数据的情况下利用模型概括数据的基本特征。这种方法与第一种方法的不同之处在于它并不关注模型是否"真实"，而只关注其是否符合已被观察到的事实。
<div class="note primary"><p>
   在社会科学研究中，我们倾向于采用第三种视角，即统计模型的主要目标在于用最简单的结构和尽可能少的参数来概括大量数据所包含的主要信息。研究者需在精确性和简洁性之间进行权衡，从而找出最佳模型。
</p></div>

## 回归分析的步骤
> 1. 问题的表述
> 2. 变量的选择 - 专家论文、前人研究
> 3. 数据的收集
> 4. 模型设定 - 散点图、表达形式、基本假定
> 5. 参数估计/模型拟合 - （加权）最小二乘法、极大似然法、岭估计法、主成分估计法
> 6. 模型检验 - 假设检验、回归诊断
> 7. 模型评价 - 拟合优度、交叉验证
> 8. 解释预测

------

## 简单线性回归模型
### 模型设定1--表达形式
开门见山，写出数学表达式：
$$
y_i=\beta_0 + \beta_1 x_i + \epsilon_i
$$
<div class="note info"><p>
   这里：<br>
   Y 是一个随机变量；<br>
   X 虽被称作变量，但它的各个取值其实是已知的，只是其取值在不同的个体之间变动；<br>
   ε 是随机误差项，假定其为服从均值为 0、方差为 σ² 的正态分布的随机变量。
</p></div>

对应指定的 $x_i$ 值，在一定的条件下，对公式求条件期望后得到 **总体回归方程**：
$$
E(Y|X=x_i) = \mu_i = \beta_0 + \beta_1x_i
$$
它表示，对于每一个特定的取值 $x_i$，观测值 y 实际上都来自一个均值为 μ、方差为 σ² 的正态分布，回归直线将穿过点 $(x_i,\mu_i)$，$\beta_0$ 就是回归直线在 y 轴上的截距，而 $\beta_1$ 则是回归直线的斜率。如下图所示：

![3_1 回归直线.PNG](https://blog-1255524710.cos.ap-beijing.myqcloud.com/images/3_1 回归直线.PNG)

无论回归模型还是回归方程，都是针对总体而言，是对总体恃征的总结和描述。所以，参数 $\beta_0$ 和 $\beta_1$ 也是总体的特征。当利用样本统计量 $b_0$ 和 $b_1$ 代替总体回归方程中的 $\beta_0$ 和 $\beta_1$ 时，就得到了 **估计的回归方程或经验回归方程**，其形式为：
$$
\hat{y} = b_0 + b_1x_i
$$
同时，我们也可以得到观测值与估计值之差，称为残差，记作 $e_i$，它对应的是公式中的总体随机误差项 $\epsilon_i$，观测值、估计值和残差这三者之间的关系可用下图加以说明。

![3_2 经验回归直线.PNG](https://blog-1255524710.cos.ap-beijing.myqcloud.com/images/3_2 经验回归直线.PNG)

<br>

------

### 模型设定2--基本假定[诸多错误，日后修正]
可以看到在第一步的模型设定中我们首先做出了线性假定方可建立线性回归模型，同时假定 ε 的分布以便于确定 $y_i$ 的分布，接下来着重阐释简单线性回归模型中的诸多假定及这些假定存在的必要性。

#### 线性假定
该假定规定 Y 的条件均值是自变量 X 的线性函数：
$$
E(Y|X=x_i)=\mu_i = \beta_0 + \beta_1 x_i
$$
<div class="note primary"><p>
   这里“线性”有双重含义，一方面可解释为变量 Y 和 X 之间是线性的，另一方面也可解释为回归函数关于参数是线性的。在某些情况下，我们可能会碰到非线性函数的情形。借助于数学上的恒等变换，我们有时可以将非线性函数转换成线性函数的形式。
</p></div>

例如，对于 $y_i = \alpha x_i^\gamma \sigma_i$，通过变换可以得到:
$$
\ln y_i = \beta_0 + \beta_1 \ln x_i + \epsilon_i
$$
其中， $\beta_0=\ln \alpha ,\beta_1=\gamma ,\epsilon_i=\ln\sigma_i$。经过转换后的方程便可以运用最小二乘法，并使得估计值仍然保持最小二乘法估计值的性质。

#### 正交假定
正交假定包括：
> 1. 误差项 ε 和 x 不相关，即 $Cov(X,\epsilon)=0$ ;
> 2. 误差项 ε 的期望值为0，即 $E(\epsilon)=0$ ;
> 推导： 由正交假定可得 $Cov(\hat{y},\epsilon)=0$ 。

在线性假定和正交假定下，可以将简单线性回归方程中 y 的条件期望定义为:
$$
E(Y|x)=\beta_0 + \beta_1 x
$$
正交假定是一个关键的识别假定，它帮助我们从条件期望 E(Y|x) 中剥离出误差项。在这假定下，**利用最小二乘估计得到的 β0 和 βl 的估计值 b0 和 b1 是无偏的**[详见下方-各随机变量的分布]，即：
$$
\begin{align*}
 E(b_0)&=\beta_0 \\ 
 & \\
 E(b_1)&=\beta_1 
\end{align*}
$$
这一假定是最小二乘估计的计算的理论依据，所以 **最小二乘估计的结果一定无例外地满足如下公式**:
$$
\begin{cases}
 &\sum_{i=1}^{n}e_i=0 \\ 
 & \\
 &\sum_{i=1}^{n}x_ie_i=0 
\end{cases}
\rightleftharpoons 
\begin{cases}
 & \frac{\partial D}{\partial b_0} = -2\sum_{i=1}^{n}(y_i-b_0-b_1x_i) = 0 \\ 
 & \\
 & \frac{\partial D}{\partial b_1} = -2\sum_{i=1}^{n}x_i(y_i-b_0-b_1x_i) = 0
\end{cases}
$$

#### 独立同分布假定
也称 i.i.d 假定，是指误差项 ε 相互独立，并且遵循同一分布，有：
$$
\begin{align*}
 \sigma_\epsilon^2 &= \sigma^2 \\ 
 & \\
 Cov(\epsilon_i,\epsilon_j)&=0,\left (i\neq j  \right ) 
\end{align*}
$$
<div class="note info"><p>
   尽管在没有 i. i. d 假定的情况下，最小二乘估计已经可以满足无偏性和一致性，但是同时满足前三个假定时，最小二乘估计值将是总体参数的"最佳线性无偏估计值"，也就是通常所说的BLUE (best linear unbiased estimator)。这里，"最佳"表示"最有效"，即抽样标准误最小。
</p></div>

#### 正态分布假定
尽管 i.i.d 假定规定误差项 ε 独立且同分布，但是它仍然无法确定 ε 的实际分布。不过，对于大样本数据，我们可以根据中心极限定理对 β 进行统计推断。然而在小样本情况下，我们只有在假定 ε 服从正态分布时才能使用 t检验
$$
\epsilon_i \sim N(0,\sigma^2)
$$
此时，最小二乘估计与总体参数的最大似然估计(MLE) 结果一致(Lehmann & CaseUa, 1998),也就是说 b0 和 b1 不仅是 β0 和 β1 的最佳线性无偏估计，而且是所有的 β0 和 β1 (线性和非线性的)无偏估计中的最佳选择。
<div class="note primary"><p>
   之所以这么说是因为在所有无偏估计中，最大似然估计是最佳无偏估计值。需要注意的是，由于最大似然估计可以是非线性的，因此最大似然解释的有效性将比最小二乘解释的有效性更广。进一步讲，最大似然估计的统计推断在大样本情况下具有渐近性质。也就是说，当样本规模趋于无穷大时，最大似然估计不仅满足一致性(渐近无偏)，而且能够取得一致估计量中的最小方差。
</p></div>

<br>

------

### 参数估计--最小二乘估计
回顾一下，前文建立的线性回归模型如下：
$$
y_i = \beta_0 + \beta_1x_i + \epsilon_i = E(y_i|x_i) + \epsilon_i = \mu_i + \epsilon_i
$$
在样本数据下得到拟合回归模型如下：
$$
y_i = b_0 + b_1x_i + e_i = \hat{y_i} + e_i
$$
各关系如图所示：

![封面3.PNG](https://blog-1255524710.cos.ap-beijing.myqcloud.com/cover/封面3.PNG)

最小二乘估计的基本思路便是 **找到一条与所有现有数据垂直距离平方和最小的直线**，即确保观测值和预测值的残差平方和最小：
$$
D = \sum_{i=1}^{n}e_i^2 = \sum_{i=1}^{n}(y_i-\hat{y_i})^2 = \sum_{i=1}^{n}(y_i-b_0-b_1x_i)^2
$$
<div class="note info"><p>
   注意是残差平方和而非误差平方和！！！<br>
   这是因为当使用总体数据时得到的与所有观测点垂直距离平方和最小的直线便是总体回归线；当使用样本数据时得到的与所有观测点垂直距离平方和最小的直线只能是估计回归线。所以误差与总体回归线相匹配，残差与估计回归线相匹配，在样本数据下，我们得到的只能是估计回归线，最小二乘法中的"最小"也只能是"残差平方和最小"。
</p></div>

为使残差平方和达到最小值，有：
$$
\begin{cases}
 & \frac{\partial D}{\partial b_0} = -2\sum_{i=1}^{n}(y_i-b_0-b_1x_i) = 0 \\ 
 & \\
 & \frac{\partial D}{\partial b_1} = -2\sum_{i=1}^{n}x_i(y_i-b_0-b_1x_i) = 0
\end{cases}
$$
整理得正态方程组：
$$
\begin{cases}
 & nb_0 + b_1\sum_{i=1}^{n}x_i = \sum_{i=1}^{n}y_i \\ 
 & \\
 & b_0\sum_{i=1}^{n}x_i + b_1\sum_{i=1}^{n}x_i^2 = \sum_{i=1}^{n}x_iy_i
\end{cases}
$$
求解得：
$$
\begin{align*}
 b_0 &= \frac{\sum x_i^2 \sum y_i - \sum x_i \sum x_iy_i}{n \sum x_i^2 - (\sum x_i)^2} \\
 & \\
 b_1 &= \frac{n \sum x_iy_i - \sum x_i \sum y_i}{n \sum x_i^2 - (\sum x_i)^2} \\ 
 & \\
 &= \frac{\sum (x_i - \bar{x})(y_i - \bar{y})}{\sum (x_i - \bar{x})^2} \\ 
\end{align*}
$$
在实际计算时，我们通常使用下列两个化简后的式子：
$$
\begin{align*}
 b_1 &= \frac{\sum (x_i - \bar{x})(y_i - \bar{y})}{\sum (x_i - \bar{x})^2} \\ 
 & \\
 &= \frac{\sum (x_i - \bar{x})(y_i - \bar{y})/(n-1)}{\sum (x_i - \bar{x})^2/(n-1)} \\
 & \\
 &= S_{xy}/S_x^2 \\
 &\\
 &= r\frac{S_y}{S_x} \\
 & \\
 b_0 &= \bar{y} - b_1\bar{x}
\end{align*}
$$

<br>

------

### 小结--各随机变量的分布
在上面的运算中，我们接触到的随机变量有 $\epsilon$、$y_i$、$b_0$、$b_1$ 等，接下来我们对这些随机变量的分布做个总结。

#### $\epsilon$
根据简单线性回归的假定前提可知：$\epsilon \sim N(0,\sigma^2)$

#### $y_i$
由关系式 $y_i=\beta_0+\beta_1x_i+\epsilon_i$ 及 $\epsilon$ 的分布推知：$y_i \sim N(\beta_0+\beta_1x_i,\sigma^2)$

#### $b_0$ 、 $b_1$
求解 $b_0$ 和 $b_1$ 的分布需要一些技巧，首先对 $b_0$ 和 $b_1$ 的表达式稍加变形：
$$
\begin{cases}
 b_0&=\frac{\sum x_i^2 \sum y_i - \sum x_i \sum x_iy_i}{n \sum x_i^2 - (\sum x_i)^2} \\ 
 & \\
 &=\sum \left [ \frac{1}{n}-\frac{(x_i-\bar{x})\bar{x}}{\sum (x_i-\bar{x})^2} \right ]y_i \\
 & \\
 b_1&=\frac{\sum (x_i - \bar{x})(y_i - \bar{y})}{\sum (x_i - \bar{x})^2} \\
 & \\
 &=\sum \frac{x_i-\bar{x}}{\sum (x_i-\bar{x})^2}y_i
\end{cases}
$$
可以看到，当 $x_i$ 确定，等式右端仅有 $y_i$ 为随机变量，并且现在 $y_i$ 的分布已知，$b_0$ 和 $b_1$ 的分布自然可以求出：

- 期望
$$
\begin{align*}
 E(b_1)&= \sum \frac{x_i-\bar{x}}{\sum (x_i-\bar{x})^2} E(y_i) \\ 
 & \\
 &= \sum \frac{x_i-\bar{x}}{\sum (x_i-\bar{x})^2} (\beta_0+\beta_1x_i) \\
 & \\
 &= \sum \frac{x_i-\bar{x}}{\sum (x_i-\bar{x})^2} (\beta_0+\beta_1x_i) \leftarrow \beta_0=\bar{y}-\beta_1\bar{x} \bigstar  \\
 & \\
 &= \frac{\sum \left [ (x_i-\bar{x})\bar{y}+\beta_1 (x_i-\bar{x})^2 \right ]}{\sum (x_i-\bar{x})^2}
 & \\
 &= \beta_1
\end{align*}
$$

$\bigstar$：我们知道对样本数据有 $b_0=\bar{y}-b_1\bar{x}$，同时可推导出在总体数据中存在以下关系式：
$$
\begin{cases}
 &\beta_0 = y_i-\beta_1x_i-\epsilon_i \\ 
 & \\
 & \Rightarrow  E(\beta_0)=E(y_i-\beta_1x_i-\epsilon_i)  \\ 
 & \\
 &\Rightarrow  \beta_0=E(y)-\beta_1E(x_i) 
\end{cases}
$$
<div class="note info"><p>
   需要注意的是：<br>
   在第一个式子中，x 和 y 使用的是样本数据，得出的均值是样本均值；<br>
   在第二个式子中，x 和 y 使用的是总体数据，得出的期望是总体均值。<br>
   由于样本均值是总体均值的无偏估计，所以我们在推导 b1 的期望时使用的 β0 代换式是成立的。
</p></div>

再次应用这个性质 ——— 无偏估计，可以很容易的得到 $b_0$ 的期望：
$$
\begin{align*}
 E(b_0)&=E(\bar{y}-b_1\bar{x}) \\ 
 & \\
 &= E(y)-\beta_1E(x) \\
 & \\
 &= \beta_0
\end{align*}
$$

- 方差
$$
\begin{align*}
 Var(b_0)&= \sum  \left \{ \left [ \frac{1}{n}-\frac{(x_i-\bar{x})\bar{x}}
  {\sum (x_i-\bar{x})^2} \right ]^2 Var(y_i) \right \}  \\ 
 & \\
 &= \sigma^2 \sum \left [ \frac{1}{n}-\frac{(x_i-\bar{x})\bar{x}}
  {\sum (x_i-\bar{x})^2} \right ]^2 \\ 
 & \\
 &= \sigma^2 \sum \left [ \frac{1}{n^2}-\frac{(x_i-\bar{x})\bar{x}}{2n\sum (x_i-\bar{x})^2}+
  \frac{(x_i-\bar{x})^2\bar{x}^2}{\sum (x_i-\bar{x})^2} \right ] \\
 & \\
 &= \sigma^2 \left [ \frac{1}{n}+\frac{\bar{x}^2}{\sum (x_i-\bar{x})^2} \right ] \\
 & \\
 & \\
 Var(b_1)&= \sum \left \{ \left [ \frac{x_i-\bar{x}}{\sum (x_i-\bar{x})^2} \right ]^2 Var(y_i) \right \} \\ 
 & \\
 &= \sigma^2 \frac{\sum (x_i-\bar{x})^2}{[\sum (x_i-\bar{x})^2]^2}  \\ 
 & \\
 &= \frac{\sigma^2}{\sum (x_i-\bar{x})^2}
\end{align*}
$$

可以看到在 $\epsilon$、$y_i$、$b_0$、$b_1$ 的分布中都有 $\sigma^2$ 的身影，但 $\sigma^2$ 目前还属于未知参数，所以接下来要做的就是构造 $\sigma^2$ 的估计量 $\hat{\sigma}^2$ 。


#### $\hat{\sigma}^2$ 
由于 $\sigma^2$ 是总体中误差的方差，自然想到用样本中误差（即残差）的方差来做无偏估计：
$$
\hat{\sigma}^2 = \frac{\sum e_i^2}{n-2} = \frac{\sum (y_i-\hat{y_i})^2}{n-2}
 = \frac{SSE}{n-2}
$$

这里，$n-2$ 为总体误差方差的自由度。因为我们需要以回归直线为基准来计算 $e_i$ ( 即以 $y_i-\hat{y}_i$ 进行估计)，而决定这条直线需要估计截距和斜率两个参数，所以消耗了两个自由度。

<br>

------

### 模型检验--假设检验
#### 模型整体检验

#### 回归系数检验



<br>

------

### 模型评价--拟合优度


<br>

------


### 模型预测--响应值、响应均值


<br>

------

## 前景展望
1. 需要提醒的是 OLS 回归方法找出的是两个变量间最佳的线性关系，但实际情况中两个变量间可能并不是简单的线性关系。因为社会现象往往受到诸多因素的共同影响，单因素造成某一社会现象的情况几乎不存在。但是，了解简单回归的原理是学习多元回归乃至其他更复杂统计方法的基础。
2. 同时，在简单线性回归中除了最小二乘法还有多种进行参数估计的方法可以运用，其中最大似然法的思想其实是与最小二乘法相通的。
3. 此外，尽管我们可以根据基本假定估计出回归模型，但是我们不知道这些假定是否成立。诊断数据仍然是必不可少的一个环节。这部分内容我们将在之后的系列 **回归诊断** 中进行详细讨论。

> ### 参考文章
> [1]谢宇著.回归分析[M].北京：社会科学文献出版社.2010.
> [2]（美）查特吉著.例解回归分析 原书第5版[M].北京：机械工业出版社.2013.